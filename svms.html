<!-- File: svms.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="assets/css/styles.css" />
  <link href='https://cdn.jsdelivr.net/npm/boxicons@2.0.5/css/boxicons.min.css' rel='stylesheet'>
  <title>Support Vector Machines - Annelise Thorn</title>
</head>
<body>
  <!--===== HEADER NAVIGATION =====-->
  <header class="l-header">
    <nav class="nav bd-grid">
      <div><a href="index.html" class="nav__logo">Annelise Thorn</a></div>
    
      <div class="dropdown">
        <button class="dropbtn">Menu <i class='bx bx-chevron-down'></i></button>
        <div class="dropdown-content">
          <a href="introduction.html">Introduction</a>
          <a href="dataprep_eda.html">Data Prep/EDA</a>
          <a href="clustering.html">Clustering</a>
          <a href="pca.html">PCA</a>
          <a href="naivebayes.html">Naive Bayes</a>
          <a href="dectrees.html">Decision Tree</a>
          <a href="svms.html">SVM</a>
          <a href="nn.html">Neural Net</a>
          <a href="conclusions.html">Conclusion</a>
          <a href="aboutme.html">About Me</a>
          <a href="references.html">References</a>
        </div>
      </div>
    </nav>
  </header>

  <main class="l-main">
    <section class="section" id="svms">
      <h2 class="section-title">Support Vector Machines</h2>
      <div class="svms__container bd-grid">

        <h3>Overview</h3>
        <p>
          SVMs are linear separators because they find the best line (or hyperplane) that divides different classes while maximizing the margin between them. This helps improve accuracy and generalization. The dot product is used to measure how similar two vectors are, and in SVMs, it plays a key role in the kernel function. Kernels allow SVMs to handle non-linear data by computing dot products in higher-dimensional spaces.
          For example, in a 2D plot, if Class A has points like (2, 3) and (4, 5), and Class B has (6, 1) and (8, 2), the SVM draws a line that separates them with the widest possible margin.
        </p>

        <div style="display: flex; flex-direction: column; align-items: center; gap: 2rem;">

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/svm1.png" alt="SVM 1" style="height: auto;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 1:</strong> Linear SVM Classification with Margin and Support Vectors.
              This 2D plot shows how a linear Support Vector Machine (SVM) separates Class A (red stars) and Class B (green triangles) using a decision boundary. The blue shaded region represents the margin, and the closest points from each class touching the margin are the support vectors.
            </figcaption>
          </figure>

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/svm2.png" alt="SVM 2" style="height: auto;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 2:</strong> Kernel Trick: Mapping Non-Linearly Separable Data to Higher Dimensions.
              This image illustrates the kernel trick, which allows SVMs to classify non-linearly separable data by mapping it into a higher-dimensional space. In this transformed space, a linear decision boundary can effectively separate the classes.
            </figcaption>
          </figure>

        </div>

        <h3>Data</h3>
        <p>
          This dataset comes from ClinVar and contains records of genetic variants, each labeled based on its clinical significance. For simplicity, the labels were grouped into two categories: Pathogenic (which includes terms like "Pathogenic", "Likely pathogenic", and similar) and Non-Pathogenic (everything else). Any records missing a gene name or title were excluded from the analysis.
          The features used included the length of the gene name and the title, along with a numerical representation of the gene name. The data was then split into two sets: 70% for training and 30% for testing, using a fixed random seed to ensure consistency. The training set was used to build an SVM model, and the test set was used to evaluate how well the model performed.<br>
        </p>

        <h3>Code</h3>
        <p>
          The model was implemented in Python using the scikit-learn library. Categorical gene names were encoded using LabelEncoder, and numeric features were derived from the lengths of the "Gene" and "Title" fields. A Support Vector Machine (SVM) classifier with a linear kernel was trained on the prepared dataset. The data was split into training and testing sets, and model performance was evaluated using precision, recall, and F1-score. The goal was to distinguish between pathogenic and non-pathogenic genetic variants based on simple textual and categorical features.
        </p>
        <p>
          <a href="https://github.com/anneliset47/annelisethorn.github.io/blob/main/Code/svms.py" target="_blank">View code on GitHub</a>
        </p>

        <h3>Results</h3>
        <div style="display: flex; flex-direction: column; align-items: center; gap: 2rem;">

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 300px; margin: 0;">
            <img src="assets/img/svm3.png" alt="svm3" style="max-width: 100%; height: auto;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 3:</strong> Confusion Matrices for SVM Classification Using Different Kernels (C = 1).
              This figure displays confusion matrices for SVM models trained with linear, polynomial, and RBF kernels, each using a regularization parameter of C = 1. The matrices compare predicted versus actual class labels for a binary classification task. The majority of correct predictions fall in the bottom-right cell, indicating the model's stronger ability to identify pathogenic cases (label 1). The RBF kernel achieves perfect recall for the pathogenic class but fails to identify any non-pathogenic instances.
            </figcaption>
          </figure>

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/svm4.png" alt="svm4" style="max-width: 100%; height: auto;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 4:</strong> Accuracy Comparison of SVM Models by Kernel Type and C Value.
              This table shows the classification accuracy of SVM models using three kernel types: linear, polynomial, and radial basis function (RBF). Each kernel was tested with C values of 0.1, 1.0, and 10.0. The highest accuracy of 80% was achieved by both the polynomial and RBF kernels at C = 0.1. The linear kernel maintained a consistent accuracy of 73.3% across all C values. This suggests that lower C values with non-linear kernels performed better on this dataset.
            </figcaption>
          </figure>

        </div>
        
        <!-- (e) Conclusions -->
        <h3>Conclusions</h3>
        <p>The results demonstrated that Support Vector Machines can effectively classify pathogenic genetic variants using simple textual and categorical features. Among the tested configurations, the polynomial and RBF kernels achieved the highest accuracy at lower C values, indicating better generalization under stronger regularization. The linear kernel showed consistent but lower performance. Overall, the results suggest that kernel choice and regularization have a significant impact on classification accuracy, especially in imbalanced datasets with limited features.</p>
      </div>
    </section>
  </main>
  <script src="https://unpkg.com/scrollreveal"></script>
  <script src="assets/js/main.js"></script>
</body>
</html>
