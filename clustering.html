<!-- File: clustering.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="assets/css/styles.css" />
  <link href='https://cdn.jsdelivr.net/npm/boxicons@2.0.5/css/boxicons.min.css' rel='stylesheet'>
  <title>Clustering - Annelise Thorn</title>
</head>
<body>
  <!--===== HEADER NAVIGATION =====-->
  <header class="l-header">
    <nav class="nav bd-grid">
      <div><a href="index.html" class="nav__logo">Annelise Thorn</a></div>
    
      <div class="dropdown">
        <button class="dropbtn">Menu <i class='bx bx-chevron-down'></i></button>
        <div class="dropdown-content">
          <a href="introduction.html">Introduction</a>
          <a href="dataprep_eda.html">Data Prep/EDA</a>
          <a href="clustering.html">Clustering</a>
          <a href="pca.html">PCA</a>
          <a href="arm.html">ARM</a>
          <a href="naivebayes.html">Naive Bayes</a>
          <a href="dectrees.html">Decision Tree</a>
          <a href="svms.html">SVM</a>
          <a href="regression.html">Regression</a>
          <a href="nn.html">Neural Net</a>
          <a href="conclusions.html">Conclusion</a>
          <a href="aboutme.html">About Me</a>
          <a href="references.html">References</a>
        </div>
      </div>
    </nav>
  </header>

  <main class="l-main">
    <section class="section" id="clustering">
      <h2 class="section-title">Clustering</h2>
      <div class="clustering__container bd-grid">
        <h3>Overview</h3>
        <p>
          Clustering is an unsupervised learning technique used to group similar data points together based on a distance or similarity metric.
          In this project, clustering is used to explore structure in repeat mutation data to uncover natural groupings based on repeat type, length, or genomic location.
        </p>
        <p>
          <li><strong>K-Means clustering</strong>: Divides the data into non-overlapping subsets</li>
          <li><strong>Hierarchical clustering</strong>: Builds a hierarchy of clusters</li>
        </p>
        <div class="image-row">
          <img src="assets/img/clustering_overview_1.png" alt="Clustering Concept" width="300" />
          <img src="assets/img/clustering_overview_2.png" alt="Clustering Types" width="300" />
        </div>

        <!-- (b) Data Prep -->
        <h3>Data Preparation</h3>
        <p>
          Clustering requires unlabeled numeric data. A subset of the cleaned Ensembl tandem repeat dataset, including features such as repeat length and strand (numerically encoded), was selected for this purpose.
        </p>
        <p>
          <a href="assets/data/sample_clustering_data.csv" target="_blank">Download the data sample used for clustering</a>
        </p>
        <img src="assets/img/clustering_sample_data.png" alt="Clustering Sample Data" width="500" />

        <!-- (c) Code -->
        <h3>Clustering Code</h3>
        <p>K-Means and Hierarchical clustering was applied in Python using scikit-learn and scipy. Cosine similarity was used for hierarchical clustering.</p>
        <p>
        <a href="https://github.com/anneliset47/annelisethorn.github.io/blob/main/Code/Clustering.py" target="_blank">View clustering code on GitHub</a>
        </p>
        <pre><code class="language-python">
    # K-Means clustering
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score

    k_values = [2, 3, 4, 5]
    for k in k_values:
        kmeans = KMeans(n_clusters=k, random_state=42).fit(X)
        score = silhouette_score(X, kmeans.labels_)
        print(f"k={k}, Silhouette Score={score:.3f}")

    # Hierarchical clustering with cosine similarity
    from scipy.cluster.hierarchy import linkage, dendrogram
    from scipy.spatial.distance import pdist

    cosine_dist = pdist(X, metric='cosine')
    linkage_matrix = linkage(cosine_dist, method='average')
        </code></pre>

        <!-- (d) Results -->
        <h3>Results</h3>
        <p>
          Below is a dendrogram showing hierarchical clustering results, along with cluster visualizations from K-Means using different values of <code>k</code>.
        </p>
        <img src="assets/img/dendrogram_hclust.png" alt="Hierarchical Dendrogram" width="500" />
        <img src="assets/img/kmeans_cluster_plot.png" alt="K-Means Cluster Plot" width="500" />

        <p>
          We tried <code>k = 2, 3, 4, 5</code>. The silhouette analysis (shown below) indicated that <strong>k=3</strong> yielded the most distinct clusters.
        </p>
        <img src="assets/img/silhouette_scores.png" alt="Silhouette Score Plot" width="400" />

        <p>
          The hierarchical clustering suggested a similar grouping structure, also supporting <strong>3 clusters</strong> based on the dendrogram cut line. Overall, K-Means and H-Clust produced compatible clusters with similar biological interpretations.
        </p>

        <!-- (e) Conclusions -->
        <h3>Conclusions</h3>
        <p>
          Clustering helped uncover structure in the repeat variant data, showing that certain repeat types and lengths tend to group together â€” which may reflect different classes of genomic behavior or clinical importance. This exploratory step informed feature engineering and potential subgroup targeting in downstream classification tasks.
        </p>

      </div>
    </section>