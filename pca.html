<!-- File: pca.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="assets/css/styles.css" />
  <link href='https://cdn.jsdelivr.net/npm/boxicons@2.0.5/css/boxicons.min.css' rel='stylesheet'>
  <title>PCA - Annelise Thorn</title>
</head>
<body>
  <!--===== HEADER NAVIGATION =====-->
  <header class="l-header">
    <nav class="nav bd-grid">
      <div><a href="index.html" class="nav__logo">Annelise Thorn</a></div>
    
      <div class="dropdown">
        <button class="dropbtn">Menu <i class='bx bx-chevron-down'></i></button>
        <div class="dropdown-content">
          <a href="introduction.html">Introduction</a>
          <a href="dataprep_eda.html">Data Prep/EDA</a>
          <a href="clustering.html">Clustering</a>
          <a href="pca.html">PCA</a>
          <a href="arm.html">ARM</a>
          <a href="naivebayes.html">Naive Bayes</a>
          <a href="dectrees.html">Decision Tree</a>
          <a href="svms.html">SVM</a>
          <a href="regression.html">Regression</a>
          <a href="nn.html">Neural Net</a>
          <a href="conclusions.html">Conclusion</a>
          <a href="aboutme.html">About Me</a>
          <a href="references.html">References</a>
        </div>
      </div>
    </nav>
  </header>

  <main class="l-main">
    <section class="section" id="pca">
      <h2 class="section-title">Principal Component Analysis (PCA)</h2>
      <div class="pca__container bd-grid">
        <h3>Overview</h3>
        <p>
        Principal Component Analysis (PCA) is a linear dimensionality reduction technique that transforms a dataset with possibly correlated variables into a new set of uncorrelated variables known as principal components.
        Each component captures as much variance from the data as possible.
        At the core of PCA are eigenvalues and eigenvectors of the data's covariance matrix. Eigenvectors determine the direction of the new feature space (principal components), while eigenvalues tell us how much variance is explained by each component.
        Dimensionality reduction is crucial when dealing with high-dimensional datasets. It simplifies modeling, improves visualization, and reduces overfitting by retaining only the most informative components. PCA is especially helpful when many features are noisy or correlated.
        </p>
        <div class="image-row">
        <img src="assets/img/pca_variance.png" alt="PCA Variance Explained" width="300" />
        <img src="assets/img/pca_projection.png" alt="PCA Projection Concept" width="300" />
        </div>

        <!-- (b) Data Prep -->
        <h3>Data Preparation</h3>
        <p>
        PCA requires numeric, standardized, unlabeled data. In this project, PCA is applied to the cleaned Ensembl repeat data like repeat length and strand orientation. All features were scaled before PCA was performed.
        </p>
        <p>
        <a href="assets/data/sample_pca_data.csv" target="_blank">Download sample PCA dataset</a>
        </p>
        <img src="assets/img/pca_sample_data.png" alt="PCA Sample Data" width="500" />

        <!-- (c) Code -->
        <h3>Code</h3>
        <p>
        The PCA implementation was done in Python using scikit-learn. The full code is available in the GitHub repo linked below.
        </p>
        <p>
        <a href="https://github.com/anneliset47/annelisethorn.github.io/blob/main/Code/PCA.py" target="_blank">View PCA code on GitHub</a>
        </p>
        
        <!-- (d) Results -->
        <h3>Results</h3>
        <p>
        The PCA transformed the dataset into two principal components that capture the majority of the variance. Below are visualizations of the variance explained and the projection of data points onto the first two principal components.
        </p>
        <img src="assets/img/pca_variance_ratio_plot.png" alt="Explained Variance Ratio" width="500" />
        <img src="assets/img/pca_scatter_plot.png" alt="PCA Projection Scatter" width="500" />

        <p>
        The explained variance plot shows that the first component captures the majority of the signal, while the scatter plot reveals natural groupings or gradients in the data. These groupings may correspond to different repeat types or chromosomes.
        </p>

        <!-- (e) Conclusions -->
        <h3>Conclusions</h3>
        <p>
        PCA revealed the dominant dimensions of variability in the tandem repeat dataset. It reduced complexity while preserving important structure, enabling easier visualization and interpretation. This helps inform downstream tasks such as clustering or classification by identifying key features.
        </p>

    </div>
    </section>