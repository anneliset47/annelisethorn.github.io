<!-- File: pca.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="assets/css/styles.css" />
  <link href='https://cdn.jsdelivr.net/npm/boxicons@2.0.5/css/boxicons.min.css' rel='stylesheet'>
  <title>PCA - Annelise Thorn</title>
</head>
<body>
  <!--===== HEADER NAVIGATION =====-->
  <header class="l-header">
    <nav class="nav bd-grid">
      <div><a href="index.html" class="nav__logo">Annelise Thorn</a></div>
    
      <div class="dropdown">
        <button class="dropbtn">Menu <i class='bx bx-chevron-down'></i></button>
        <div class="dropdown-content">
          <a href="introduction.html">Introduction</a>
          <a href="dataprep_eda.html">Data Prep/EDA</a>
          <a href="clustering.html">Clustering</a>
          <a href="pca.html">PCA</a>
          <a href="arm.html">ARM</a>
          <a href="naivebayes.html">Naive Bayes</a>
          <a href="dectrees.html">Decision Tree</a>
          <a href="svms.html">SVM</a>
          <a href="regression.html">Regression</a>
          <a href="nn.html">Neural Net</a>
          <a href="conclusions.html">Conclusion</a>
          <a href="aboutme.html">About Me</a>
          <a href="references.html">References</a>
        </div>
      </div>
    </nav>
  </header>

  <main class="l-main">
    <section class="section" id="pca">
      <h2 class="section-title">Principal Component Analysis (PCA)</h2>
      <div class="pca__container bd-grid">
        <h3>Overview</h3>
        <p>
        Principal Component Analysis (PCA) is a linear dimensionality reduction technique that transforms a dataset with possibly correlated variables into a new set of uncorrelated variables known as principal components.
        Each component captures as much variance from the data as possible.
        At the core of PCA are eigenvalues and eigenvectors of the data's covariance matrix. Eigenvectors determine the direction of the new feature space (principal components), while eigenvalues tell us how much variance is explained by each component.
        Dimensionality reduction is crucial when dealing with high-dimensional datasets. It simplifies modeling, improves visualization, and reduces overfitting by retaining only the most informative components. PCA is especially helpful when many features are noisy or correlated.
        </p>
        <div class="image-row">
        <img src="assets/img/pca.png" alt="PCA Variance Explained" width="300" />
        <img src="assets/img/Eigenvalues.png" alt="PCA Projection Concept" width="300" />
        </div>

        <!-- (b) Data Prep -->
        <h3>Data Preparation</h3>
        <p>
        PCA requires numeric, standardized, unlabeled data. In this project, PCA is applied to the cleaned Ensembl repeat data like repeat length and strand orientation. All features were scaled before PCA was performed.
        </p>
        <p>
        <a href="assets/Datasets/cleaned_ensembl.csv" target="_blank">Download sample PCA dataset</a>
        </p>
        <img src="assets/img/cleaned_ensembl_snapshot.png" alt="PCA Sample Data" width="500" />

        <!-- (c) Code -->
        <h3>Code</h3>
        <p>
        The PCA implementation was done in Python using scikit-learn. The full code is available in the GitHub repo linked below.
        </p>
        <p>
        <a href="https://github.com/anneliset47/annelisethorn.github.io/blob/main/Code/PCA.py" target="_blank">View PCA code on GitHub</a>
        </p>

        <!-- (d) Results -->
        <h3>PCA Results</h3>

        <figure>
        <img src="assets/img/pca_projection_sampled.png" alt="PCA Projection" />
        <figcaption>
            <strong>PCA Projection</strong><br>
            Data projected onto first two principal components.<br>
            Helps visualize clusters and variance separation.
        </figcaption>
        </figure>

        <figure>
        <img src="assets/img/pca_variance_ratio_sampled.png" alt="Explained Variance by PCA Components" />
        <figcaption>
            <strong>Explained Variance by PCA Components</strong><br>
            PC1 explains ~53%, PC2 ~47%. Two components sufficient for strong representation.
        </figcaption>
        </figure>
        
        <!-- (e) Conclusions -->
        <h3>Conclusions</h3>
        <p>
          Principal Component Analysis (PCA) successfully reduced the tandem repeat dataset to two main components while preserving nearly all of its original variance. This strong retention suggests that key features, like repeat length and strand orientation, play a major role in explaining the structure of the data. When visualized, the PCA projection showed clear separation between data points, which supports the idea that the dataset is well-organized and a good fit for unsupervised learning methods such as clustering.
          The way the clusters appeared in PCA space also aligned with the subgroup patterns identified by both K-Means and hierarchical clustering. In this way, PCA not only made the dataset easier to interpret and visualize but also helped highlight important patterns that differentiate different repeat regions. These results reinforce the value of dimensionality reduction in genomic feature analysis and lay the groundwork for future predictive modeling with clinical data.
        </p>
      </div>
    </section>