<!-- File: pca.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="assets/css/styles.css" />
  <link href='https://cdn.jsdelivr.net/npm/boxicons@2.0.5/css/boxicons.min.css' rel='stylesheet'>
  <title>PCA - Annelise Thorn</title>
</head>
<body>
  <!--===== HEADER NAVIGATION =====-->
  <header class="l-header">
    <nav class="nav bd-grid">
      <div><a href="index.html" class="nav__logo">Annelise Thorn</a></div>
    
      <div class="dropdown">
        <button class="dropbtn">Menu <i class='bx bx-chevron-down'></i></button>
        <div class="dropdown-content">
          <a href="introduction.html">Introduction</a>
          <a href="dataprep_eda.html">Data Prep/EDA</a>
          <a href="clustering.html">Clustering</a>
          <a href="pca.html">PCA</a>
          <a href="arm.html">ARM</a>
          <a href="naivebayes.html">Naive Bayes</a>
          <a href="dectrees.html">Decision Tree</a>
          <a href="svms.html">SVM</a>
          <a href="regression.html">Regression</a>
          <a href="nn.html">Neural Net</a>
          <a href="conclusions.html">Conclusion</a>
          <a href="aboutme.html">About Me</a>
          <a href="references.html">References</a>
        </div>
      </div>
    </nav>
  </header>

  <main class="l-main">
    <section class="section" id="pca">
      <h2 class="section-title">Principal Component Analysis (PCA)</h2>
      <div class="pca__container bd-grid">
        <h3>Overview</h3>
        <p>
        Principal Component Analysis (PCA) is a linear dimensionality reduction technique that transforms a dataset with possibly correlated variables into a new set of uncorrelated variables known as principal components.
        Each component captures as much variance from the data as possible.
        At the core of PCA are eigenvalues and eigenvectors of the data's covariance matrix. Eigenvectors determine the direction of the new feature space (principal components), while eigenvalues tell us how much variance is explained by each component.
        Dimensionality reduction is crucial when dealing with high-dimensional datasets. It simplifies modeling, improves visualization, and reduces overfitting by retaining only the most informative components. PCA is especially helpful when many features are noisy or correlated.
        </p>
        <div class="image-row">
        <img src="assets/img/pca.png" alt="PCA Variance Explained" width="300" />
        <img src="assets/img/Eigenvalues.png" alt="PCA Projection Concept" width="300" />
        </div>

        <!-- (b) Data Prep -->
        <h3>Data Preparation</h3>
        <p>
        PCA requires numeric, standardized, unlabeled data. In this project, PCA is applied to the cleaned Ensembl repeat data like repeat length and strand orientation. All features were scaled before PCA was performed.
        </p>
        <p>
        <a href="assets/Datasets/cleaned_ensembl.csv" target="_blank">Download sample PCA dataset</a>
        </p>
        <img src="assets/img/pca_sample_data.png" alt="PCA Sample Data" width="500" />

        <!-- (c) Code -->
        <h3>Code</h3>
        <p>
        The PCA implementation was done in Python using scikit-learn. The full code is available in the GitHub repo linked below.
        </p>
        <p>
        <a href="https://github.com/anneliset47/annelisethorn.github.io/blob/main/Code/PCA.py" target="_blank">View PCA code on GitHub</a>
        </p>

        <!-- (d) Results -->
        <h3>PCA Results</h3>

        <figure>
        <img src="assets/img/pca_projection_sampled.png" alt="PCA Projection" />
        <figcaption>
            <strong>PCA Projection</strong><br>
            Data projected onto first two principal components.<br>
            Helps visualize clusters and variance separation.
        </figcaption>
        </figure>

        <figure>
        <img src="assets/img/pca_variance_ratio_sampled.png" alt="Explained Variance by PCA Components" />
        <figcaption>
            <strong>Explained Variance by PCA Components</strong><br>
            PC1 explains ~53%, PC2 ~47%. Two components sufficient for strong representation.
        </figcaption>
        </figure>
    </div>
    </section>