<!-- File: dectrees.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="assets/css/styles.css" />
  <link href='https://cdn.jsdelivr.net/npm/boxicons@2.0.5/css/boxicons.min.css' rel='stylesheet'>
  <title>Decision Tree - Annelise Thorn</title>
</head>
<body>
  <!--===== HEADER NAVIGATION =====-->
  <header class="l-header">
    <nav class="nav bd-grid">
      <div><a href="index.html" class="nav__logo">Annelise Thorn</a></div>
      <div class="dropdown">
        <button class="dropbtn">Menu <i class='bx bx-chevron-down'></i></button>
        <div class="dropdown-content">
          <a href="introduction.html">Introduction</a>
          <a href="dataprep_eda.html">Data Prep/EDA</a>
          <a href="clustering.html">Clustering</a>
          <a href="pca.html">PCA</a>
          <a href="naivebayes.html">Naive Bayes</a>
          <a href="dectrees.html">Decision Tree</a>
          <a href="svms.html">SVM</a>
          <a href="nn.html">Neural Net</a>
          <a href="conclusions.html">Conclusion</a>
          <a href="aboutme.html">About Me</a>
          <a href="references.html">References</a>
        </div>
      </div>
    </nav>
  </header>

  <main class="l-main">
    <section class="section" id="decision-tree">
      <h2 class="section-title">Decision Tree</h2>
      <div class="decision-tree__container bd-grid">

        <!-- (a) Overview -->
        <h3>Overview</h3>
        <p>A decision tree is a model that makes predictions by asking a series of questions based on features in the data.
        It begins at the top, called the root, and moves down the branches by choosing paths based on the answers until it reaches a leaf node that provides the final prediction.
        Decision trees are popular because they are easy to understand and can be used for both classification and regression tasks.
        During training, the model figures out the best questions to ask by looking for features that split the data into the purest possible groups, where purity means that most or all items in a group belong to the same class.
        To decide how good a split is, the model uses a splitting criterion like Gini impurity or entropy.
        Gini impurity measures how often a randomly chosen item would be incorrectly labeled if labels were assigned at random based on the class distribution, with a value of 0 meaning perfect purity.
        Entropy also measures how mixed the classes are and is used to calculate information gain, which shows how much uncertainty is reduced by a split.
        A higher information gain means a better split, so the model selects features that give the greatest gain.
        Once trained, the model makes predictions by checking values of features at each node and moving through the tree until a final decision is reached.
        This process allows the tree to classify new data based on the patterns it learned during training.</p>

        <div style="display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap; align-items: stretch;">

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/decisiontree1.png" alt="Decision Tree 1" style="height: 250px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 1:</strong> Visual structure of a decision tree. This diagram shows how a decision tree is made up of decision nodes, leaf nodes, and branches. The tree starts at the root node, asks questions at each decision node, and ends at a leaf node that gives the final prediction.
            </figcaption>
          </figure>

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/decisiontree2.png" alt="Decision Tree 2" style="height: 250px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 2:</strong> CComparison of Gini impurity and entropy. This graph shows how both metrics measure the impurity of a dataset. Entropy increases faster than Gini and reaches its maximum when classes are evenly mixed. These values help the tree decide where to split the data.
            </figcaption>
          </figure>

        </div>

        <!-- (b) Data Preparation -->
        <h3>Data Preparation</h3>
        <p>Supervised learning methods require labeled datasets structured into features and targets. The dataset used in this project was derived from ClinVar and contains genetic variant records with associated clinical significance. The label, or target variable, was derived from the "ClinicalSignificance" column and was simplified into two categories: "Pathogenic" and "Non-Pathogenic." All entries marked as "Pathogenic," "Likely pathogenic," "Pathogenic/Likely pathogenic," or "Pathogenic; risk factor" were categorized as "Pathogenic." All other designations were grouped as "Non-Pathogenic."
        Only records with values in both the "Gene" and "Phenotypes" columns were retained. These two columns were selected as features and encoded numerically using label encoding to make them suitable for decision tree modeling.
        The resulting dataset was divided into a training set and a testing set using a 70/30 split. Stratified sampling was applied to preserve class distribution across both sets. The training set was used exclusively for model construction, while the testing set was used only for evaluation. These sets were kept disjoint to prevent data leakage and ensure objective performance measurement.</p>

        <figure style="text-align: center; max-width: 100%; margin: 2rem auto;">
          <img src="assets/img/sample_data_table_dt.png" alt="Sample Data Table Snapshot" style="max-width: 100%; height: 250px;" />
          <figcaption style="font-size: 0.95rem; color: #444; margin-top: 0.5rem;">
            <strong>Figure 3:</strong> A sample of the data used to train and test the decision tree model. Each row contains a genetic variant, with encoded values for the associated gene and phenotype, along with the binary classification label indicating pathogenicity.
          </figcaption>
        </figure>

        <!-- (c) Code -->
        <h3>Code</h3>
        <p>The model was implemented in Python using the scikit-learn library. Categorical features were encoded using LabelEncoder, and a DecisionTreeClassifier was trained using the training set. Three versions of the decision tree were created using different depth configurations: a shallow tree (max depth = 2), a medium-depth tree (max depth = 4), and a fully expanded tree with no depth limit.
        Each model was trained on the same data and evaluated using accuracy and a confusion matrix. The features used were numeric representations of "Gene" and "Phenotypes."</p>
        <p>
          <a href="https://github.com/anneliset47/annelisethorn.github.io/blob/main/Code/dectree.py" target="_blank">View code on GitHub</a>
        </p>

        <!-- (d) Results -->
        <h3>Results</h3>

        <div style="display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap; align-items: stretch;">

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/dectreeconfusionmatrix.png" alt="Confusion Matrix" style="max-width: auto; height: 300px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 4:</strong> Confusion matrix of the decision tree classifier with full depth. The model achieved 63 percent accuracy. It correctly identified 19 pathogenic variants but misclassified all non-pathogenic variants as pathogenic.
            </figcaption>
          </figure>

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/dectree2.png" alt="Decision Tree 2" style="max-width: auto; height: 300px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 5:</strong> A decision tree trained with a maximum depth of 4. Each internal node represents a decision based on gene or phenotype encoding. The model splits data to reach a leaf node, which outputs the final prediction.
            </figcaption>
          </figure>

        </div>

        <!-- (e) Conclusions -->
        <h3>Conclusions</h3>
        <p>The decision tree model successfully learned from categorical inputs to classify pathogenicity of genetic variants with moderate accuracy. The highest-performing model achieved 63 percent accuracy on the test set. The confusion matrix revealed that the model was more likely to predict variants as pathogenic, leading to several false positives.
        The trees provided interpretability by visualizing how the model made decisions based on the encoded gene and phenotype inputs. Model performance can be improved by including additional biological features such as variant type, conservation scores, or frequency data. Pruning or ensemble techniques like Random Forest may also reduce overfitting and increase generalization.
        The results suggest that decision tree classifiers can detect meaningful patterns in genomic data when predicting pathogenicity, even with a minimal set of features.</p>
      </div>
    </section>
  </main>
</body>
</html>
