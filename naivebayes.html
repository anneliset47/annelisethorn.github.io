<!-- File: naivebayes.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="assets/css/styles.css" />
  <link href='https://cdn.jsdelivr.net/npm/boxicons@2.0.5/css/boxicons.min.css' rel='stylesheet'>
  <title>Naive Bayes - Annelise Thorn</title>
</head>
<body>
  <!--===== HEADER NAVIGATION =====-->
  <header class="l-header">
    <nav class="nav bd-grid">
      <div><a href="index.html" class="nav__logo">Annelise Thorn</a></div>
      <div class="dropdown">
        <button class="dropbtn">Menu <i class='bx bx-chevron-down'></i></button>
        <div class="dropdown-content">
          <a href="introduction.html">Introduction</a>
          <a href="dataprep_eda.html">Data Prep/EDA</a>
          <a href="clustering.html">Clustering</a>
          <a href="pca.html">PCA</a>
          <a href="naivebayes.html">Naive Bayes</a>
          <a href="dectrees.html">Decision Tree</a>
          <a href="regression.html">Regression</a>
          <a href="nn.html">Neural Net</a>
          <a href="conclusions.html">Conclusion</a>
          <a href="aboutme.html">About Me</a>
          <a href="references.html">References</a>
        </div>
      </div>
    </nav>
  </header>

  <main class="l-main">
    <section class="section" id="naive-bayes">
      <h2 class="section-title">Naive Bayes</h2>
      <div class="naive-bayes__container bd-grid">

        <!-- (a) Overview -->
        <h3>Overview</h3>
        <p>Naive Bayes is a type of classification algorithm that uses probability to decide which category something belongs to. 
          It is based on Bayesâ€™ Theorem and assumes that all features are independent of each other. 
          This assumption is not always true, but the algorithm still works well in many cases. 
          Naive Bayes is commonly used for text classification, such as filtering spam emails or detecting the sentiment of a review.
          Multinomial Naive Bayes is used when the data is made up of counts. A good example is word counts in text documents.
           During training, the model learns how often each word appears in documents of each category. 
           When a new document comes in, it checks which words are in it and uses the word frequencies to figure out the most likely category.
           Sometimes, a word does not appear in the training data for a certain category. If that happens, the model gives it a probability of zero, which can ruin the whole prediction. 
           Smoothing, such as Laplace smoothing, solves this by adding a small number to each word count. 
           This makes sure that no probability is ever exactly zero, which helps the model handle new or rare words better.
           Bernoulli Naive Bayes works differently. It looks at whether a word is present or not, instead of how many times it appears. 
           This is useful for short texts or cases where just knowing if a word shows up is more helpful than counting how often it does. 
           Each word becomes a yes or no feature, depending on whether it appears in the text.
        </p>

        <div style="display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap; align-items: stretch;">

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/naivebayes1.png" alt="Naive Bayes Flowchart" style="height: 250px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 1:</strong> Step-by-step flowchart of the Naive Bayes classification process. 
              The algorithm evaluates each attribute, calculates probabilities based on feature values, updates class likelihoods, and continues through all available attributes before making a final classification decision.
            </figcaption>
          </figure>

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/naivebayes2.png" alt="Naive Bayes Smoothing" style="height: 250px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 2:</strong> Visual explanation of Laplacian smoothing. 
              This image shows how smoothing can be used to adjust sharp transitions in data. 
              Although this example focuses on curves, the idea is similar to how smoothing helps in Naive Bayes by preventing zero probabilities.
            </figcaption>
          </figure>

        </div>

        <!-- (b) Data Preparation -->
        <h3>Data Preparation</h3>
        <p>Supervised machine learning models require labeled data in a structured format. In this case, the label is the pathogenicity of each genetic variant, obtained from the "ClinicalSignificance" column in the ClinVar dataset. The original labels were simplified into two categories: "Pathogenic" and "Non-Pathogenic." Any variant labeled as "Pathogenic," "Likely pathogenic," "Pathogenic/Likely pathogenic," or "Pathogenic; risk factor" was classified as "Pathogenic." All other variants, including those labeled as "Benign" or "Uncertain significance," were grouped under "Non-Pathogenic." 
        Only records with complete entries for the "Gene" and "Phenotypes" columns were used. These two features were selected for modeling. Since the Multinomial Naive Bayes algorithm requires numerical input, both categorical features were encoded as numeric values using label encoding.
        After feature extraction and encoding, the dataset was split into training and testing sets. Seventy percent of the data was assigned to the training set, and thirty percent to the testing set. This split was stratified to maintain the original distribution of the label classes. The training set was used to fit the model, while the testing set was reserved for evaluating model performance. These two sets were kept disjoint to ensure an unbiased assessment and prevent data leakage.</p>

        <figure style="text-align: center; max-width: 100%; margin: 2rem auto;">
          <img src="assets/img/placeholder_table.png" alt="Placeholder Table Snapshot" style="max-width: 100%; height: 250px;" />
          <figcaption style="font-size: 0.95rem; color: #444; margin-top: 0.5rem;">
            <strong>Figure 3:</strong> Caption for data snapshot table image.
          </figcaption>
        </figure>

        <!-- (c) Code -->
        <h3>Code</h3>
        <p>Filler text for code section.</p>
        <p>
          <a href="#" target="_blank">View code on GitHub</a>
        </p>

        <!-- (d) Results -->
        <h3>Results</h3>

        <div style="display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap; align-items: stretch;">

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/placeholder_result1.png" alt="Placeholder Result 1" style="max-width: auto; height: 300px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 4:</strong> Caption for result image 1.
            </figcaption>
          </figure>

          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/placeholder_result2.png" alt="Placeholder Result 2" style="max-width: auto; height: 300px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 5:</strong> Caption for result image 2.
            </figcaption>
          </figure>

        </div>

        <!-- (e) Conclusions -->
        <h3>Conclusions</h3>
        <p>The Multinomial Naive Bayes model demonstrated moderate performance in predicting the pathogenicity of genetic variants using only two categorical features: gene and phenotype. An overall accuracy of 67 percent indicates that some predictive signal exists in the selected features. However, the model produced a number of false negatives, which is a limitation when attempting to detect potentially harmful variants.
        Additional improvements can be made by incorporating more informative features, such as variant type, genomic location, evolutionary conservation, and allele frequency. Further refinements could also include techniques to address class imbalance or the use of more advanced algorithms such as ensemble models or support vector machines.
        The results show that even a simple model with basic inputs can provide a reasonable starting point for predicting variant pathogenicity. However, a more comprehensive feature set is necessary to improve accuracy and reliability in a real-world application.</p>

      </div>
    </section>
  </main>
</body>
</html>
